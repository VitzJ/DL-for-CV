# -*- coding: utf-8 -*-
"""DL for CV Assignment 1 Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qzQQYsXKvfYrX6ib2o5EKl6oGrADfJKR

# DL for CV Assignment 1
## John Vitz

This assignment includes the compilation of a .zip file containing .png files generated by extracting the frames from https://www.youtube.com/watch?v=WeF4wpw7w9k utilizing ffmpeg. I commented out stuff that would be cause errors (hopefully) such as the actual process of extracting from the zip file and compliling the images into a HF dataset. I will try to fully set up the actual environment within an IDE by the next assignment. I have docker installed on linux mint, and can launch example dev containers in VS Code, but have yet to figure out how to set up / use a custom dev container.
"""

# Required on Colab (for me at least)

#!pip install datasets
#!pip install huggingface_hub datasets
#!pip install fiftyone

import zipfile
import os
from datasets import load_dataset, DatasetDict
import matplotlib.pyplot as plt
from huggingface_hub import notebook_login
import glob
import torch
from torchvision import transforms
import requests
from io import BytesIO
from PIL import Image
import fiftyone as fo
import fiftyone.zoo as foz
import tempfile

"""## Creating Dataset from ffmpeg extracted images .zip

I used the following command in ffmpeg to download all frames in video.

ffmpeg -i Downloads/Cyclist_and_vehicle_Tracking-1.mp4 Downloads/Cyclist_and_vehicle_Tracking-1_images/frame_%04d.png

I then added the files to an archive using 7zip and uploaded them to Google Colab Session Storage.
"""

#zip_path = "/content/Cyclist_and_vehicle_Tracking-1_images.zip"
#extract_path = "/content/dataset"


#with zipfile.ZipFile(zip_path, 'r') as zip_ref: # extract zip file
#    zip_ref.extractall(extract_path)

#print("Images saved at:", extract_path)

#dataset = load_dataset("imagefolder", data_dir=extract_path)

#print(dataset)

#print(dataset["train"][0]) # first item

#img = dataset["train"][0]["image"] # display first item
#plt.imshow(img)
#plt.axis("off")
#plt.show()

#notebook_login() # Prompts for my HF token ^^

"""## Adding Proper In-Order File Names"""

# sorted list of image files in the folder
#image_files = sorted(glob.glob(os.path.join(extract_path, "*.png"))) # the .zip contains .png images

#print(image_files[:5])  # first 5 filenames to verify sort order

# load dataset from the image folder
#dataset = load_dataset("imagefolder", data_dir=extract_path)

# add filenames
#def add_filenames(example, index):
#    example["file_name"] = os.path.basename(image_files[index])  # filename
#    return example

#dataset = dataset.map(add_filenames, with_indices=True)

#for i in range(5):  # Check first 5 samples
#    print(dataset["train"][i]["file_name"])

#import matplotlib.pyplot as plt

#img = dataset["train"][0]["image"] # display first item (of HF dataset)
#plt.imshow(img)
#plt.axis("off")
#plt.show()

"""## Push Dataset to Hugging Face Hub"""

#repo_id = "JohnVitz/DL_for_CV_Assignment_1"

# push dataset to the Hugging Face Hub
#dataset.push_to_hub(repo_id)

"""# Dataset Load from HuggingFace"""

dataset_from_HF = load_dataset("JohnVitz/DL_for_CV_Assignment_1")

print('\n', dataset_from_HF["train"][0])  # first image in HF dataset

img = dataset_from_HF["train"][0]["image"]  # Get the first image
plt.imshow(img)
plt.axis("off")
plt.show()

"""# Example Image"""

# The URL of the image
example_image_url = "https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/astronaut.jpg"

# Send a GET request to fetch the image, Convert the image into a variable using BytesIO, Open the image using PIL (Python Imaging Library)

example_img = Image.open(BytesIO(requests.get(example_image_url).content))

# Display the image using matplotlib
plt.imshow(example_img)
plt.axis('off')  # To hide axes
plt.show()

"""## Applying Resize Transformation to example image"""

ex_resize_transform = transforms.Resize((224, 224))

resized_example_img = ex_resize_transform(example_img)

# Display the image using matplotlib
plt.imshow(resized_example_img)
plt.axis('off')  # To hide axes
plt.show()

"""## Applying Normalization Transformation to example image"""

# Have to load image data into a tensor in order to normalize
ex_to_tensor_transform = transforms.ToTensor()

example_img_tensor = ex_to_tensor_transform(resized_example_img)
print(example_img_tensor)

print(example_img_tensor.shape)

# To normalize, find mean and std of the image.

ex_mean = example_img_tensor.mean(dim=[1, 2]) # This is found seperately per channel, and takes into account the actual X, Y of the image (224, 224) from the tensor dimensions
ex_std = example_img_tensor.std(dim=[1, 2]) # This is found seperately per channel, and takes into account the actual X, Y of the image (224, 224) from the tensor dimensions

print(f'mean: {ex_mean}')
print(f'std: {ex_std}')

# apply normalization using mean and std of the image
ex_normalize_transform = transforms.Normalize(mean=ex_mean, std=ex_std)

example_img_normalized = ex_normalize_transform(example_img_tensor)

print(example_img_normalized)

"""# Apply Resizing and Normalization across the entire video frame HF dataset"""

dataset_from_HF["train"][0]['image']

resize_transform = transforms.Resize((224, 224)) # resize to 224x224
to_tensor_transform = transforms.ToTensor() # convert to tensors

HF_dataset_tensors = [] # store image tensors here

for i, frame in enumerate(dataset_from_HF['train']): # for every image from HF dataset

    image = dataset_from_HF["train"][i]['image']  # Load image

    # Resize the image
    image_resized = resize_transform(image)

    # Convert image to tensor and store it
    image_tensor = to_tensor_transform(image_resized)
    HF_dataset_tensors.append(image_tensor)

all_images_tensors = torch.stack(HF_dataset_tensors) # Stack the tensors into a single tensor (shape: [num_images, C, H, W])

# mean and std for the entire dataset
HF_dataset_mean = all_images_tensors.mean(dim=[0, 2, 3])  # Mean per channel (C, H, W)
HF_dataset_std = all_images_tensors.std(dim=[0, 2, 3])    # Std per channel (C, H, W)

print(f"Calculated Mean: {HF_dataset_mean}")
print(f"Calculated Std: {HF_dataset_std}")

# now to normalize using calculated mean and std from above
normalize_transform = transforms.Normalize(mean=HF_dataset_mean, std=HF_dataset_std)  # Normalize based on mean and std calculated above

all_images_tensors_norm = normalize_transform(all_images_tensors)
print(all_images_tensors_norm)

"""This is actually standardization, not normalization. Because I used mean and std to convert values, I didn't force them into a hard range which is what normalization does.

# Visualizing the dataset

# Loading the Example Dataset
"""

deafult_fo_dataset = foz.load_zoo_dataset("quickstart")

print(deafult_fo_dataset)

session = fo.launch_app(deafult_fo_dataset)

session.close()

"""# Using FiftyOne with custom Video Frame HF Dataset"""

print(dataset_from_HF) # I need to load this dataset into FiftyOne. I will upload the re-sized version of it.

print(dataset_from_HF['train'])

dataset_from_HF["train"][i]['file_name']

"""As a note, I tried to just directly load images from memory into fiftyone, however I couldn't figure it out and had to resort to the tempfile package."""

# Resize operation
resize_transform = transforms.Resize((224, 224))

# List to store samples
samples = []

# Temporary directory to store resized images
temp_dir = tempfile.mkdtemp()

for i, frame in enumerate(dataset_from_HF['train']):  # for every image from HF dataset
    image = dataset_from_HF["train"][i]['image']  # Load image
    # label = dataset_from_HF["train"][i]['file_name']
    label = int(dataset_from_HF["train"][i]['file_name'][6:10]) # Load the label for given image (the file name)

    # Apply the resize transform
    image_resized = resize_transform(image)  # Resize to 224x224

    # Save the resized image to a temporary file
    temp_image_path = os.path.join(temp_dir, f"resized_image_{i}.png")
    image_resized.save(temp_image_path, format="PNG")  # Save as PNG

    # Create a sample with the file path
    sample = fo.Sample(filepath=temp_image_path)  # Use the file path for the image
    sample["label"] = label  # Add the label

    # Append to the list of samples
    samples.append(sample)

samples

# Delete the existing dataset if it exists
#fo.delete_dataset("HF_fiftyone_dataset")

# Create a FiftyOne dataset and add the samples
fo_HF_dataset = fo.Dataset(name="HF_fiftyone_dataset")
fo_HF_dataset.add_samples(samples)

# Sort by 'label' field in ascending order
fo_HF_dataset.sort_by("label")

# Sort the samples in the dataset by the 'label' field in ascending order
fo_HF_dataset.sort_by("label")

# Launch the FiftyOne app to view the sorted dataset
session = fo.launch_app(dataset=fo_HF_dataset)

session.close()

"""# End of Assignment"""
